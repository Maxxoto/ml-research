{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J-q9pkY0WLx"
      },
      "source": [
        "## Cost Function\n",
        "\n",
        "Is function to check how far the model precict wrong value (Currently we use MSE) and Find weight and bias with optimizer using Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "oAg3NvTIHbnX",
        "outputId": "1a43d527-af36-4492-95a1-32612fbfe15d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Actual values (ground truth)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m7.0\u001b[39m])\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Actual values (ground truth)\n",
        "y_true = np.array([3.0, -0.5, 2.0, 7.0])\n",
        "\n",
        "# Predicted values by model\n",
        "y_pred = np.array([2.5, 0.0, 2.1, 7.8])\n",
        "\n",
        "# MSE function\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Calculate\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "errors = y_true - y_pred\n",
        "squared_errors = errors ** 2\n",
        "\n",
        "plt.bar(range(len(y_true)), squared_errors, color='skyblue')\n",
        "plt.title(\"Squared Errors per Example\")\n",
        "plt.xlabel(\"Data Point\")\n",
        "plt.ylabel(\"Squared Error\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sayLK_TJI_o"
      },
      "source": [
        "##Test Lab : Linear Regression using 2 Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qre6QEJnWY7g"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Prediction:  ≈∑ = wx + b\n",
        "Cost:        MSE = (1/n) * Œ£(y - ≈∑)^2\n",
        "Goal:        Find best w and b that minimize MSE\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "QL_jHWDmJTMQ",
        "outputId": "17df2156-5654-4d72-b9ae-1956db5e5b80"
      },
      "outputs": [],
      "source": [
        "# üìç Step 1: Import Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# üìç Step 2: Simulate Some Data\n",
        "# Let's say this is house size (in square feet)\n",
        "X = np.array([500, 750, 1000, 1250, 1500])  # input feature\n",
        "y = np.array([150000, 220000, 250000, 320000, 370000])  # price in $\n",
        "# y_easy = np.array([150000, 200000, 250000, 300000, 350000])  # price in $ This dataset is easy to predict because the pattern so obvious\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.xlabel(\"Size (sqft)\")\n",
        "plt.ylabel(\"Price ($)\")\n",
        "plt.title(\"House Price vs Size\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txuFJDf9W4qv"
      },
      "source": [
        "## Using manual parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "g_u99PGjJV5A",
        "outputId": "c811aaa4-63de-469a-acbd-d07710dc545a"
      },
      "outputs": [],
      "source": [
        "# üìç Step 3: Define the Prediction Function\n",
        "# y_pred = w * x + b\n",
        "\n",
        "def predict(X, w, b):\n",
        "    return w * X + b\n",
        "\n",
        "# üìç Step 4: Define the Cost Function (MSE)\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# üìç Step 5: Try a Sample Prediction and Compute Cost\n",
        "\n",
        "# Try some random weights (Hyperparameter) that we need to tune based on model (this is random) can be manually tuned or we can find best weight and bias\n",
        "# Using gradient descent\n",
        "\n",
        "w = 200  # $200 per sqft\n",
        "b = 50000  # base price\n",
        "\n",
        "y_pred = predict(X, w, b)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "print(f\"Predicted Prices: {y_pred}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "\n",
        "# üìç Step 6: Plot Prediction Line vs True Data\n",
        "plt.scatter(X, y, color='blue', label='Actual')\n",
        "plt.plot(X, y_pred, color='red', label='Prediction')\n",
        "plt.xlabel(\"Size (sqft)\")\n",
        "plt.ylabel(\"Price ($)\")\n",
        "plt.title(\"Prediction vs Actual\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "8bb8d01b",
        "outputId": "90156a13-cfc0-46a5-d606-16dfbf2c7d62"
      },
      "outputs": [],
      "source": [
        "# Add the new square footage values to the existing X array\n",
        "X_new = np.array([2200, 3300, 3500])\n",
        "X_combined = np.concatenate((X, X_new))\n",
        "\n",
        "print(X_combined)\n",
        "\n",
        "# Use the predict function with the current w and b to get predictions for the new sizes\n",
        "y_pred_new = predict(X_new, w, b)\n",
        "\n",
        "print(f\"New Square Footage Values: {X_new}\")\n",
        "print(f\"Predicted Prices for New Sizes: {y_pred_new}\")\n",
        "\n",
        "# You can also visualize these new predictions on the plot\n",
        "plt.scatter(X, y, color='blue', label='Actual (Original)')\n",
        "plt.scatter(X_new, y_pred_new, color='green', label='Predicted (New)') # Plot new predictions\n",
        "plt.plot(X_combined, predict(X_combined, w, b), color='red', label='Prediction Line') # Plot the full line\n",
        "plt.xlabel(\"Size (sqft)\")\n",
        "plt.ylabel(\"Price ($)\")\n",
        "plt.title(\"Prediction vs Actual (with New Predictions)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSkoMvFNfV9Z"
      },
      "source": [
        "## Optimizer Implementation using Gradient Descent\n",
        "\n",
        "In gradient descent you only need to tune learning rate and epoch (iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UyqE5oWGkood",
        "outputId": "0f486c1c-f98b-4aa3-db8e-ceb16a2d4d38"
      },
      "outputs": [],
      "source": [
        "# Gradient computation\n",
        "def compute_gradients(x, y, y_pred):\n",
        "    n = len(x)\n",
        "    dw = (-2 / n) * np.sum(x * (y - y_pred))\n",
        "    db = (-2 / n) * np.sum(y - y_pred)\n",
        "    return dw, db\n",
        "\n",
        "def normalize(array):\n",
        "    \"\"\"Normalize to mean 0 and std 1\"\"\"\n",
        "    return (array - np.mean(array)) / np.std(array)\n",
        "\n",
        "# ----------------------------\n",
        "# Training Function\n",
        "# ----------------------------\n",
        "\n",
        "def train(x, y, learning_rate=0.01, epochs=100):\n",
        "    w, b = 0.0, 0.0\n",
        "    cost_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        y_pred = predict(x, w, b)\n",
        "        cost = mean_squared_error(y, y_pred)\n",
        "        dw, db = compute_gradients(x, y, y_pred)\n",
        "\n",
        "        # Update weights\n",
        "        w -= learning_rate * dw\n",
        "        b -= learning_rate * db\n",
        "\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Cost = {cost:.4f}, w = {w:.4f}, b = {b:.4f}\")\n",
        "\n",
        "    return w, b, cost_history\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main Execution\n",
        "# ----------------------------\n",
        "\n",
        "# Normalize input and output\n",
        "X_norm = normalize(X)\n",
        "y_norm = normalize(y)\n",
        "\n",
        "# Train the model\n",
        "w, b, costs = train(X_norm, y_norm, learning_rate=0.01, epochs=1000)\n",
        "\n",
        "print(f\"\\nüéØ Final model: y = {w:.4f}x + {b:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Visualize Cost Over Time\n",
        "# ----------------------------\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(costs)\n",
        "plt.title(\"Cost over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Visualize Fit\n",
        "# ----------------------------\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.scatter(X_norm, y_norm, label='Data')\n",
        "plt.plot(X_norm, predict(X_norm, w, b), color='red', label='Model')\n",
        "plt.title(\"Linear Fit After Training\")\n",
        "plt.xlabel(\"Normalized X\")\n",
        "plt.ylabel(\"Normalized y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92d3Bamvpwyp"
      },
      "source": [
        "## This use case above is good , but too many epoch so we waste a lot of computational resource , in next step i will implement \"Early Stop\" so it will stop when the `cost` is optimal enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e08b286e",
        "outputId": "effb4c80-b8e4-42fd-ea9b-eb59d036d7bf"
      },
      "outputs": [],
      "source": [
        "# Gradient computation\n",
        "def compute_gradients(x, y, y_pred):\n",
        "    n = len(x)\n",
        "    dw = (-2 / n) * np.sum(x * (y - y_pred))\n",
        "    db = (-2 / n) * np.sum(y - y_pred)\n",
        "    return dw, db\n",
        "\n",
        "def normalize(array):\n",
        "    \"\"\"Normalize to mean 0 and std 1\"\"\"\n",
        "    return (array - np.mean(array)) / np.std(array)\n",
        "\n",
        "# ----------------------------\n",
        "# Training Function\n",
        "# ----------------------------\n",
        "\n",
        "def train(x, y, learning_rate=0.01, epochs=100, early_stop_threshold=1e-6, patience=10):\n",
        "    w, b = 0.0, 0.0\n",
        "    cost_history = []\n",
        "    best_cost = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        y_pred = predict(x, w, b)\n",
        "        cost = mean_squared_error(y, y_pred)\n",
        "        dw, db = compute_gradients(x, y, y_pred)\n",
        "\n",
        "        # Update weights\n",
        "        w -= learning_rate * dw\n",
        "        b -= learning_rate * db\n",
        "\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Patience-based early stopping\n",
        "        if best_cost - cost < early_stop_threshold:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"‚èπÔ∏è Early stopping at epoch {epoch}\")\n",
        "                print(f\"üìâ Cost = {cost:.6f}, w = {w:.4f}, b = {b:.4f}\")\n",
        "                break\n",
        "        else:\n",
        "            best_cost = cost\n",
        "            patience_counter = 0\n",
        "\n",
        "        # Log every x epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"üìâ Epoch {epoch}: Cost = {cost:.6f}, w = {w:.4f}, b = {b:.4f}\")\n",
        "\n",
        "    # Ensure return always happens\n",
        "    return w, b, cost_history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main Execution\n",
        "# ----------------------------\n",
        "\n",
        "# Normalize input and output\n",
        "X_norm = normalize(X)\n",
        "y_norm = normalize(y)\n",
        "\n",
        "# Train the model\n",
        "w, b, costs = train(X_norm, y_norm, learning_rate=0.01, epochs=1000, early_stop_threshold=1e-5, patience=5)\n",
        "\n",
        "\n",
        "print(f\"\\nüéØ Final model: y = {w:.4f}x + {b:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Visualize Cost Over Time\n",
        "# ----------------------------\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(costs)\n",
        "plt.title(\"Cost over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Visualize Fit\n",
        "# ----------------------------\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.scatter(X_norm, y_norm, label='Data')\n",
        "plt.plot(X_norm, predict(X_norm, w, b), color='red', label='Model')\n",
        "plt.title(\"Linear Fit After Training\")\n",
        "plt.xlabel(\"Normalized X\")\n",
        "plt.ylabel(\"Normalized y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml-research",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
